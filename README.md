# VISTA: Visually Impaired Speech Technology Assistant

**VISTA** is an AI-powered browser extension designed to enable visually impaired users to navigate and interact with websites using natural voice commands. Developed during the **Meta AI Llama Hackathon 2024** in Toronto, VISTA combines advanced web scraping, large language models, and speech technology to provide seamless, conversational access to web content.

## ðŸš€ Motivation

The web remains largely inaccessible to visually impaired individuals, especially when websites are poorly structured or lack assistive tagging. VISTA was built to address this gapâ€”empowering users to obtain relevant information through voice in a natural, intuitive manner.

By integrating a transformer-based web scraper, the **Llama 3.1-8B** model, and **Retrieval-Augmented Generation (RAG)**, VISTA delivers context-aware answers extracted directly from any webpage. Paired with **speech-to-text** and **text-to-speech** capabilities, it enables fluid conversation with the web, enhancing independence and accessibility.

## ðŸš€ Getting Started

### Prerequisites
- Python 3.9+
- [Ollama](https://ollama.com) (for running Llama 3.1 models locally)
- Chrome or any Chromium-based browser (for extension deployment)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/vista-ai-assistant.git
   cd vista-meta-hackathon
   ```
2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```


## ðŸ’» Technology Stack
<p align="center">
  <a href="https://go-skill-icons.vercel.app/">
    <img
      src="https://go-skill-icons.vercel.app/api/icons?i=python,javascript,fask,langchain,ollama,git"
    />
  </a>
</p>
<p align="center">
